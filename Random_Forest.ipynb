{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/ML-DL-Algorithm-Notes/blob/master/Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5IiqlFMIkfn",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> Random Forest  </center></h1>\n",
        "\n",
        "- We've seen that decision trees can be powerful supervised machine learning models. However, they're not without their weakness= decision trees are often prone to overfitting. We've discussed some strategies to minimize this problem, like pruning, but sometimes that isn't enough. We need to find another way to generalize our trees. During this practice,  I will show how the **random forest** work.\n",
        "\n",
        "\n",
        "- A **random forest** is an ensemble machine learning technique - a random forest contains many decision trees that all work together to classifiy new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It's like every tree gets a vote, and the most popular classification wins. \n",
        "\n",
        "\n",
        "- Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb-Kf5FWgqkn",
        "colab_type": "text"
      },
      "source": [
        "**Bagging at Random Forest**\n",
        "\n",
        "Trees in a random forest classifier are created by using a random subset of the original dataset with replacement. This process is known as bagging. Bagging prevents overfitting, given that each individual tree is trained on a subset of original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv8ZFEegg7Wh",
        "colab_type": "text"
      },
      "source": [
        "**Random Foreset definition**\n",
        "\n",
        "A Random Forest Classifier is an ensemble machine learning model that uses multiple unique decision trees to classify unlabeled. If compared to an individual decision tree, Random Forest is a more robust classifier but its interpretability is reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZsKmwRbhLqn",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest feature consideration**\n",
        "\n",
        "When creating a decision tree in a random forest, a random subset of features are considered as the best feature to split the data on. By splitting the data in a random subset of features, all estimators are trained considering different aspects of the data, which reduces the probability of overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWiWy4-3hiz0",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest aggregative performace**\n",
        "\n",
        "A random forest classifier makes its classification by taking an aggregate of the classifications from all the trees in the random forest. For classification, this aggregate is a majority vote. For regression, this could be the average of the trees in the random forest. This aggregation allows the classifier to capture complex non-linear relations from the data. The model performance is far superior than a linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0AfioUYh7-D",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest overfitting**\n",
        "\n",
        "Random Forests are used to avoid overfitting. By aggregating the classification of multiple trees, having overfitted trees in the random forest is less impactful. Reducing overfiting translates to greater generalization capacity, which increases classification accuracy on new unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWSZNy-3JjXw",
        "colab_type": "text"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-nxh0LzKD6o",
        "colab_type": "text"
      },
      "source": [
        "With the boostrap method, we pick N (the number of rows in training set) rows at random with replacement. We pick K times (number of trees)."
      ]
    }
  ]
}