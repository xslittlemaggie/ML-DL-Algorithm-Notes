{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree from scratch to complete & Limitations.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xslittlemaggie/Other-ML-DL-Algorithm-notes/blob/master/Decision_Tree_from_scratch_to_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRGZZBUX7PnQ",
        "colab_type": "text"
      },
      "source": [
        "<h1><center> Realization of Decesion Tree from Scratch </center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBiW1j5lv7G5",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Calculate gini impurity\n",
        "The lower the better, more pure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8HvuPmv13n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def gini(labels):\n",
        "    impurity = 1\n",
        "    label_counts = Counter(labels)\n",
        "    for label in label_counts:\n",
        "      probability_of_label = label_counts[label]/len(labels)\n",
        "      impurity -= probability_of_label ** 2\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhZKKvCDwP6z",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Calculate information gain"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxnjKZ6t35f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def information_gain(starting_labels, split_labels):\n",
        "  info_gain = gini(starting_labels)\n",
        "  for subset in split_labels:\n",
        "    info_gain -= gini(subset) * len(subset)/len(starting_labels)\n",
        "  return info_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apkI8QfLtrmG",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Split data based on different features (column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ2AxXSm3SQi",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 split data based on each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ6_GPJuqyS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cars = [['med', 'low', '3', '4', 'med', 'med'], \n",
        "        ['med', 'vhigh', '4', 'more', 'small', 'high'], \n",
        "        ['high', 'med', '3', '2', 'med', 'low'], \n",
        "        ['med', 'low', '4', '4', 'med', 'low'], \n",
        "        ['med', 'low', '5more', '2', 'big', 'med'], \n",
        "        ['med', 'med', '2', 'more', 'big', 'high'], \n",
        "        ['med', 'med', '2', 'more', 'med', 'med'], \n",
        "        ['vhigh', 'vhigh', '2', '2', 'med', 'low'], \n",
        "        ['high', 'med', '4', '2', 'big', 'low'], \n",
        "        ['low', 'low', '2', '4', 'big', 'med']]\n",
        "\n",
        "\n",
        "car_labels = ['acc', 'acc', 'unacc', 'unacc', 'unacc', 'vgood', 'acc', 'unacc', 'unacc', 'good']\n",
        "\n",
        "def split(dataset, labels, column):\n",
        "    data_subsets = []\n",
        "    label_subsets = []\n",
        "    counts = list(set([data[column] for data in dataset]))\n",
        "    counts.sort()\n",
        "    for k in counts:\n",
        "        new_data_subset = []\n",
        "        new_label_subset = []\n",
        "        for i in range(len(dataset)):\n",
        "            if dataset[i][column] == k:\n",
        "                new_data_subset.append(dataset[i])\n",
        "                new_label_subset.append(labels[i])\n",
        "        data_subsets.append(new_data_subset)\n",
        "        label_subsets.append(new_label_subset)\n",
        "    return information_gain(labels, label_subsets)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvs_cVbkuxth",
        "colab_type": "code",
        "outputId": "5391bb34-9efc-40a0-bc65-0f0d48d59d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "for i in range(6):\n",
        "  data_subsets, label_subsets = split(cars, car_labels, i)\n",
        "  print(\"The information gain for feature {} is: {:.4f}.\".format(i + 1, information_gain(car_labels, label_subsets)))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The information gain for feature 1 is: 0.2733.\n",
            "The information gain for feature 2 is: 0.0400.\n",
            "The information gain for feature 3 is: 0.1067.\n",
            "The information gain for feature 4 is: 0.3067.\n",
            "The information gain for feature 5 is: 0.1500.\n",
            "The information gain for feature 6 is: 0.2900.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtywFsz23Rzy",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 split data based on best feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd7M1OPW3Z7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(dataset, labels):\n",
        "    best_gain = 0\n",
        "    best_feature = 0\n",
        "    for feature in range(len(dataset[0])):\n",
        "        data_subsets, label_subsets = split(dataset, labels, feature)\n",
        "        gain = information_gain(labels, label_subsets)\n",
        "        if gain > best_gain:\n",
        "            best_gain, best_feature = gain, feature\n",
        "    return best_feature, best_gain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpbn51UL3-QR",
        "colab_type": "code",
        "outputId": "ff028d00-ad7b-4508-e85d-6fa8b0786fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "find_best_split(cars, car_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 0.3066666666666667)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Jl9_QXzu6q",
        "colab_type": "text"
      },
      "source": [
        "Thus, from the result above, when split the data with the fourth feature (indexed from 0), the information gain is highest. This is the best feature to split on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztaFgSyZ0X8I",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Recursive Tree Building\n",
        "Now that we can find the best feture to split the dataset, we can repeate this process again and again to create the full tree. This is cursive algorithm! We start with every data point from the trinign set, find the best feature to split the data, split the data based on that feature, and then recursively repeat the process again on each subset that was created from the split.\n",
        "\n",
        "We'll stop the recursion when we can no longer find a feature that results in any information gain. In other words, we want to create a leaf of the tree when we can't find a way to split the data that makes purer subsets.\n",
        "\n",
        "The leaf should keep track of the classes of the data points from the trining set that ended up in the leaf. In our inplementation, we'll use a Counter object to keep track of the counts of labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-oQgjoGu9pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tree(data, labels):\n",
        "  best_feature, best_gain = find_best_split(data, labels)\n",
        "  if best_gain == 0:\n",
        "    return Counter(labels)\n",
        "  data_subsets, label_subsets = split(data, labels, best_feature)\n",
        "  branches = []\n",
        "  for i in range(len(data_subsets)):\n",
        "    tree = build_tree(data_subsets[i], label_subsets[i])\n",
        "    branches.append(tree)\n",
        "  return branches\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}